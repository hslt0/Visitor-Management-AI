# Visitor Management AI Module (Offline RAG)

This module implements an offline, privacy-first AI query capability for Visitor Management data. It allows users to ask natural language questions (e.g., *"Who is currently in unit ABC?"*) and receive accurate answers based strictly on the provided database context.

## 1. System Architecture

The system is built on the **Retrieval-Augmented Generation (RAG)** pattern using the **Model Context Protocol (MCP)** approach.

- **Language Model**: Microsoft Phi-4-mini-instruct (via ONNX Runtime GenAI).
- **Runtime**: Pure .NET 10 (C#).
- **Data Access**: Secure Native Tool Calling (No direct DB access for AI).
- **Inference**: CPU-based, fully offline.

## 2. Security & Tenant Isolation Guarantees

This module is designed for multi-tenant environments where strict data isolation is critical.

### 2.1 Isolation Mechanism
Cross-tenant data leakage is prevented at the architectural level. The AI model **never** determines the scope of data access.

1.  **Context Injection**: The `SiteId` (Tenant ID) is retrieved from the authenticated user context in the Controller.
2.  **Hardcoded Filters**: When the AI requests a tool (e.g., `find_visitor`), the C# backend intercepts the call and injects the `SiteId` directly into the SQL `WHERE` clause.
3.  **Verification**:
    * *AI Intent*: `{"query": "Alex"}`
    * *Executed SQL*: `SELECT * FROM Checkins WHERE VisitorName LIKE '%Alex%' AND SiteId = @InjectedSiteId`
    
    It is technically impossible for the AI to override the `@InjectedSiteId` parameter via prompt injection.

### 2.2 Offline Constraints & Privacy
* **Zero Egress**: All inference runs locally on the CPU. No data (prompts, records, or logs) is transmitted to external cloud providers (e.g., OpenAI, Azure).
* **Hallucination Control**: The system implements strict post-processing logic to verify visitor presence (checking `CheckoutTimestamp`) before generating the answer.

## 3. Model Setup & Installation

### 3.1 Prerequisites
To download the model, install the **Hugging Face CLI** via PowerShell:

```powershell
powershell -ExecutionPolicy ByPass -c "irm [https://hf.co/cli/install.ps1](https://hf.co/cli/install.ps1) | iex"

```

### 3.2 Installation

Navigate to the `AI/Models/Phi-4-mini-cpu` directory (create if it doesn't exist) and execute the following command to download the quantized ONNX model (~5 GB):

```powershell
hf download microsoft/Phi-4-mini-instruct-onnx --include "cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/*" --local-dir . --local-dir-use-symlinks False

```

### 3.3 Directory Structure

After the download is complete, verify that the files are organized as follows (flattened structure):

```text
AI/
└── Models/
    └── Phi-4-mini-cpu/
        ├── model.onnx
        ├── genai_config.json
        ├── tokenizer.json
        ├── tokenizer_config.json
        └── ...

```

### 3.4 Application Configuration

Update your `appsettings.json` to point to the current model directory:

```json
{
  "AiSettings": {
    "ModelPath": "AI/Models/Phi-4-mini-cpu"
  }
}

```

## 4. API & Output Formats

The API returns a structured object `VisitorQueryResponse` to support both Chat UI and programmatic integration (e.g., Email reporting).

**Endpoint:** `POST /api/home/ask`

**Response Structure:**

```json
{
  "answer": "Alex Nilsson is currently here (checked in at 19:37).",
  "dataContext": "[{\"VisitorName\": \"Alex Nilsson\", ...}]",
  "toolUsed": "find_visitor",
  "timestamp": "2026-01-10T17:00:00"
}

```

* `answer`: Natural language summary generated by AI (for Chat UI).
* `dataContext`: Raw JSON data retrieved from the database (Source of Truth, suitable for **Email Reports**).
* `toolUsed`: Metadata about which data tool was employed.

## 5. Technical Requirements

* **Runtime**: Fully offline inference (no internet required).
* **RAM**: 8 GB minimum (16 GB recommended).
* **OS**: Windows and Linux (Ubuntu) compatible.
* **Concurrency**: Service is registered as `Singleton` to handle multi-threaded requests efficiently using a shared model instance.

---